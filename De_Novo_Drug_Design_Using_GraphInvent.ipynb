{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u3lIVRTggdF"
      },
      "source": [
        "#**GraphINVENT**: Graph Networks for Molecular Design\n",
        "Deep learning  applied to chemistry can be used to accelerate the discovery of new molecules.\n",
        "GraphINVENT is a platform developed for **graph-based molecular design** using\n",
        "**graph neural networks (GNNs)**. GraphINVENT uses a deep neural network architecture to probabilistically generate new molecules a single bond at a time.\n",
        "\n",
        "The models implemented in GraphINVENT can **quickly learn** to build **molecules resembling training set molecules** without any explicit programming of chemical rules.\n",
        "\n",
        "The framework is implemented in **PyTorch** and provides high modularity, making it easy to experiment with different graph neural network (GNN) architectures.\n",
        "\n",
        "Full Paper: [ChemRxiv](https://chemrxiv.org/engage/chemrxiv/article-details/60c74f19ee301c084fc7a627)\n",
        "GitHub: [Repo](https://github.com/MolecularAI/GraphINVENT)\n",
        "\n",
        "\n",
        "# How it works.\n",
        "\n",
        "1. **Preprocessing (Graph Deconstruction)**:\n",
        "\n",
        "\n",
        "*   Molecules from the dataset are deconstructed step-by-step into smaller subgraphs using a modified breadth-first search (BFS).\n",
        "\n",
        "*   This process defines a decoding route (r = ((G0, APD0), ..., (Gn, APDn))) that instructs the model how to reconstruct the molecule from an empty graph.\n",
        "*   Each subgraph step is associated with an Action Probability Distribution (APD): probabilities over possible actions (add atom, form bond, terminate).\n",
        "\n",
        "2. **Model Architecture**\n",
        "\n",
        "\n",
        "\n",
        "*   Two core blocks:\n",
        "      **GNN block (6 types: MNN, GGNN, S2V, AttGGNN, AttS2V, EMN)**: learns node and edge-level features. **Global readout block**: computes the APD for the entire graph using learned node features and embeddings.\n",
        "*   **The APD contains three vectors:** fadd: add a new node with bond. fconn: connect last node to another. fterm: terminate graph generation.\n",
        "3. **Training**: APDs are the model’s outputs, and the training goal is to minimize the KL-divergence between predicted and target APDs. Uses mini-batches and Adam optimizer. Trained using datasets like GDB-13 and MOSES.\n",
        "4. **Generation**: Starts from an empty graph.\n",
        "In each step, samples an action from the APD (add/connect/terminate).\n",
        "Continues until a termination signal is generated or an invalid action occurs (e.g., exceeding max atoms).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7fMZcjXU2_q"
      },
      "source": [
        "# Preparing the environment\n",
        "\n",
        "**Miniconda Installation**: with Python 3.8, suitable for compatibility with GraphINVENT's dependencies.\n",
        "\n",
        "**Conda Initialization and Updates**: Initializes the Conda shell and update all Conda packages for a stable environment.\n",
        "\n",
        "**System Path Update**: Add the newly installed Python packages to the system path to ensure modules are discoverable by Colab.\n",
        "\n",
        "**GraphINVENT Cloning**: The GraphINVENT repository is cloned from GitHub.\n",
        "\n",
        "**Environment Activation**: The Conda environment defined in graphinvent.yml is created and activated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1L2gmtegjFG"
      },
      "outputs": [],
      "source": [
        "################## Installing Conda #####################\n",
        "\n",
        "%env PYTHONPATH=\n",
        "! rm -rf Miniconda3-py38_23.1.0-1-Linux-x86_64.sh.* # In case this is run multiple times\n",
        "! wget https://repo.anaconda.com/miniconda/Miniconda3-py38_23.1.0-1-Linux-x86_64.sh\n",
        "# ! wget https://repo.anaconda.com/archive/Anaconda3-2022.10-Linux-x86_64.sh\n",
        "! chmod +x Miniconda3-py38_23.1.0-1-Linux-x86_64.sh\n",
        "! bash ./Miniconda3-py38_23.1.0-1-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "! conda install --channel defaults conda python=3.8 --yes\n",
        "! conda update --channel defaults --all --yes\n",
        "\n",
        "import sys\n",
        "sys.path.append('/usr/local/lib/python3.8/site-packages/')\n",
        "\n",
        "! conda init bash\n",
        "\n",
        "################ Cloning GraphINVENT ####################\n",
        "\n",
        "! rm -rf ./GraphINVENT # In case this is run multiple times\n",
        "! git clone https://github.com/literalEval/GraphINVENT.git\n",
        "%cd GraphINVENT\n",
        "! ls\n",
        "\n",
        "################ Activating our environment #############\n",
        "\n",
        "! conda env create -f environments/graphinvent.yml\n",
        "! source ~/.bashrc\n",
        "! source activate graphinvent\n",
        "! eval \"$(conda shell.bash hook)\"\n",
        "! source ~/.bashrc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lOiV3w-9I-hO"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pwd\n",
        "source activate graphinvent\n",
        "conda install ipykernel -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDngQXN1c-Vc"
      },
      "source": [
        "<h4> Summary of Datasets Used in GraphINVENT</h4>\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>Dataset</th>\n",
        "      <th>Train/Test/Val Size</th>\n",
        "      <th>Max Nodes (|Vmax|)</th>\n",
        "      <th>Atom Types</th>\n",
        "      <th>Formal Charges</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>GDB-13 1K (rand)</td>\n",
        "      <td>1K / 1K / 1K</td>\n",
        "      <td>12K</td>\n",
        "      <td>{C, N, O, S, Cl}</td>\n",
        "      <td>{-1, 0, +1}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>GDB-13 1K (canon)</td>\n",
        "      <td>1K / 1K / 1K</td>\n",
        "      <td>11K–12K</td>\n",
        "      <td>{C, N, O, S, Cl}</td>\n",
        "      <td>{-1, 0, +1}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>MOSES (rand)</td>\n",
        "      <td>1.5M / 176K / 10K</td>\n",
        "      <td>33M</td>\n",
        "      <td>{C, N, O, F, S, Cl, Br}</td>\n",
        "      <td>{0}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>MOSES (canon)</td>\n",
        "      <td>1.5M / 176K / 10K</td>\n",
        "      <td>26M</td>\n",
        "      <td>{C, N, O, F, S, Cl, Br}</td>\n",
        "      <td>{0}</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>MOSES (arom)</td>\n",
        "      <td>1.5M / 176K / 10K</td>\n",
        "      <td>40M</td>\n",
        "      <td>{C, N, O, F, S, Cl, Br}</td>\n",
        "      <td>{0}</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "\n",
        "*   GDB-13 1K: A small subset of the GDB-13 database, used for fast testing and hyperparameter tuning. “rand” and “canon” refer to random or canonical node ordering during preprocessing.\n",
        "*   MOSES: A curated subset of the ZINC database, focused on drug-like molecules.\n",
        "\n",
        "Three versions are used:\n",
        "\n",
        "rand: random deconstruction.\n",
        "\n",
        "canon: canonical deconstruction.\n",
        "\n",
        "arom: includes aromatic bond types explicitly.\n",
        "\n",
        "# Preparing a new dataset\n",
        "\n",
        "Once you have selected a dataset to study, you must prepare it so that it agrees with the format expected by the program. GraphINVENT expects, for each dataset, three splits in SMILES format. Each split should be named as follows:\n",
        "\n",
        "- train.smi\n",
        "- test.smi\n",
        "- valid.smi\n",
        "\n",
        "These should contain the training set, test set, and validation set, respectively. It is not important for the SMILES to be canonical, and it also does not matter if the file has a header or not. How many structures you put in each split is also up to you (generally the training set is larger than the testing and validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bA_vg2xJwOVX"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pwd\n",
        "source activate graphinvent\n",
        "\n",
        "python3.8\n",
        "\"\"\"\n",
        "Example submission script for a GraphINVENT training job (distribution-\n",
        "based training, not fine-tuning/optimization job). This can be used to\n",
        "pre-train a model before a fine-tuning (via reinforcement learning) job.\n",
        "\n",
        "To run, type:\n",
        "(graphinvent) ~/GraphINVENT$ python submit-pre-training.py\n",
        "\"\"\"\n",
        "# load general packages and functions\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# define what you want to do for the specified job(s)\n",
        "DATASET          = \"gdb13_1K-debug\"    # dataset name in \"./data/pre-training/\"\n",
        "JOB_TYPE         = \"train\"             # \"preprocess\", \"train\", \"generate\", or \"test\"\n",
        "JOBDIR_START_IDX = 0                   # where to start indexing job dirs\n",
        "N_JOBS           = 1                   # number of jobs to run per model\n",
        "RESTART          = False               # whether or not this is a restart job\n",
        "FORCE_OVERWRITE  = True                # overwrite job directories which already exist\n",
        "JOBNAME          = \"example-job-name\"  # used to create a sub directory\n",
        "\n",
        "# if running using SLURM sbatch, specify params below\n",
        "USE_SLURM = False                        # use SLURM or not\n",
        "RUN_TIME  = \"1-00:00:00\"                 # hh:mm:ss\n",
        "MEM_GB    = 20                           # required RAM in GB\n",
        "\n",
        "# for SLURM jobs, set partition to run job on (preprocessing jobs run entirely on\n",
        "# CPU, so no need to request GPU partition; all other job types benefit from running\n",
        "# on a GPU)\n",
        "if JOB_TYPE == \"preprocess\":\n",
        "    PARTITION     = \"core\"\n",
        "    CPUS_PER_TASK = 1\n",
        "else:\n",
        "    PARTITION     = \"gpu\"\n",
        "    CPUS_PER_TASK = 4\n",
        "\n",
        "# set paths here\n",
        "HOME             = str(Path.home())\n",
        "PYTHON_PATH      = \"/usr/local/envs/graphinvent/bin/python3.8\"\n",
        "GRAPHINVENT_PATH = \"./graphinvent/\"\n",
        "DATA_PATH        = \"./data/pre-training/\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "# define dataset-specific parameters\n",
        "params = {\n",
        "    \"atom_types\"   : [\"C\", \"N\", \"O\", \"S\", \"Cl\"],\n",
        "    \"formal_charge\": [-1, 0, +1],\n",
        "    \"max_n_nodes\"  : 13,\n",
        "    \"job_type\"     : JOB_TYPE,\n",
        "    \"dataset_dir\"  : f\"{DATA_PATH}{DATASET}/\",\n",
        "    \"restart\"      : RESTART,\n",
        "    \"model\"        : \"GGNN\",\n",
        "    \"sample_every\" : 2,\n",
        "    \"init_lr\"      : 1e-4,\n",
        "    \"epochs\"       : 100,\n",
        "    \"batch_size\"   : 50,\n",
        "    \"block_size\"   : 1000,\n",
        "    \"device\"       : DEVICE,\n",
        "    \"n_samples\"    : 100,\n",
        "    # additional paramaters can be defined here, if different from the \"defaults\"\n",
        "    # for instance, for \"generate\" jobs, don't forget to specify \"generation_epoch\"\n",
        "    # and \"n_samples\"\n",
        "}\n",
        "\n",
        "\n",
        "def submit() -> None:\n",
        "    \"\"\"\n",
        "    Creates and submits submission script. Uses global variables defined at top\n",
        "    of this file.\n",
        "    \"\"\"\n",
        "    check_paths()\n",
        "\n",
        "    # create an output directory\n",
        "    dataset_output_path = f\"{HOME}/GraphINVENT/output_{DATASET}\"\n",
        "    tensorboard_path    = os.path.join(dataset_output_path, \"tensorboard\")\n",
        "    if JOBNAME != \"\":\n",
        "        dataset_output_path = os.path.join(dataset_output_path, JOBNAME)\n",
        "        tensorboard_path    = os.path.join(tensorboard_path, JOBNAME)\n",
        "\n",
        "    os.makedirs(dataset_output_path, exist_ok=True)\n",
        "    os.makedirs(tensorboard_path, exist_ok=True)\n",
        "    print(f\"* Creating dataset directory {dataset_output_path}/\", flush=True)\n",
        "\n",
        "    # submit `N_JOBS` separate jobs\n",
        "    jobdir_end_idx = JOBDIR_START_IDX + N_JOBS\n",
        "    for job_idx in range(JOBDIR_START_IDX, jobdir_end_idx):\n",
        "\n",
        "        # specify and create the job subdirectory if it does not exist\n",
        "        params[\"job_dir\"]         = f\"{dataset_output_path}/job_{job_idx}/\"\n",
        "        params[\"tensorboard_dir\"] = f\"{tensorboard_path}/job_{job_idx}/\"\n",
        "\n",
        "        # create the directory if it does not exist already, otherwise raises an\n",
        "        # error, which is good because *might* not want to override data our\n",
        "        # existing directories!\n",
        "        os.makedirs(params[\"tensorboard_dir\"], exist_ok=True)\n",
        "        try:\n",
        "            job_dir_exists_already = bool(\n",
        "                JOB_TYPE in [\"generate\", \"test\"] or FORCE_OVERWRITE\n",
        "            )\n",
        "            os.makedirs(params[\"job_dir\"], exist_ok=job_dir_exists_already)\n",
        "            print(\n",
        "                f\"* Creating model subdirectory {dataset_output_path}/job_{job_idx}/\",\n",
        "                flush=True,\n",
        "            )\n",
        "        except FileExistsError:\n",
        "            print(\n",
        "                f\"-- Model subdirectory {dataset_output_path}/job_{job_idx}/ already exists.\",\n",
        "                flush=True,\n",
        "            )\n",
        "            if not RESTART:\n",
        "                continue\n",
        "\n",
        "        # write the `input.csv` file\n",
        "        write_input_csv(params_dict=params, filename=\"input.csv\")\n",
        "\n",
        "        # write `submit.sh` and submit\n",
        "        if USE_SLURM:\n",
        "            print(\"* Writing submission script.\", flush=True)\n",
        "            write_submission_script(job_dir=params[\"job_dir\"],\n",
        "                                    job_idx=job_idx,\n",
        "                                    job_type=params[\"job_type\"],\n",
        "                                    max_n_nodes=params[\"max_n_nodes\"],\n",
        "                                    runtime=RUN_TIME,\n",
        "                                    mem=MEM_GB,\n",
        "                                    ptn=PARTITION,\n",
        "                                    cpu_per_task=CPUS_PER_TASK,\n",
        "                                    python_bin_path=PYTHON_PATH)\n",
        "\n",
        "            print(\"* Submitting job to SLURM.\", flush=True)\n",
        "            subprocess.run([\"sbatch\", params[\"job_dir\"] + \"submit.sh\"],\n",
        "                           check=True)\n",
        "        else:\n",
        "            print(\"* Running job as a normal process.\", flush=True)\n",
        "            subprocess.run([\"ls\", f\"{PYTHON_PATH}\"], check=True)\n",
        "            subprocess.run([f\"{PYTHON_PATH}\",\n",
        "                            f\"{GRAPHINVENT_PATH}main.py\",\n",
        "                            \"--job-dir\",\n",
        "                            params[\"job_dir\"]],\n",
        "                           check=True)\n",
        "\n",
        "        # sleep a few secs before submitting next job\n",
        "        print(\"-- Sleeping 2 seconds.\")\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "def write_input_csv(params_dict : dict, filename : str=\"params.csv\") -> None:\n",
        "    \"\"\"\n",
        "    Writes job parameters/hyperparameters in `params_dict` to CSV using the specified\n",
        "    `filename`.\n",
        "    \"\"\"\n",
        "    dict_path = params_dict[\"job_dir\"] + filename\n",
        "\n",
        "    with open(dict_path, \"w\") as csv_file:\n",
        "\n",
        "        writer = csv.writer(csv_file, delimiter=\";\")\n",
        "        for key, value in params_dict.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "\n",
        "def write_submission_script(job_dir : str, job_idx : int, job_type : str, max_n_nodes : int,\n",
        "                            runtime : str, mem : int, ptn : str, cpu_per_task : int,\n",
        "                            python_bin_path : str) -> None:\n",
        "    \"\"\"\n",
        "    Writes a submission script (`submit.sh`).\n",
        "\n",
        "    Args:\n",
        "    ----\n",
        "        job_dir (str)         : Job running directory.\n",
        "        job_idx (int)         : Job idx.\n",
        "        job_type (str)        : Type of job to run.\n",
        "        max_n_nodes (int)     : Maximum number of nodes in dataset.\n",
        "        runtime (str)         : Job run-time limit in hh:mm:ss format.\n",
        "        mem (int)             : Gigabytes to reserve.\n",
        "        ptn (str)             : Partition to use, either \"core\" (CPU) or \"gpu\" (GPU).\n",
        "        cpu_per_task (int)    : How many CPUs to use per task.\n",
        "        python_bin_path (str) : Path to Python binary to use.\n",
        "    \"\"\"\n",
        "    submit_filename = job_dir + \"submit.sh\"\n",
        "    with open(submit_filename, \"w\") as submit_file:\n",
        "        submit_file.write(\"#!/bin/bash\\n\")\n",
        "        submit_file.write(f\"#SBATCH --job-name={job_type}{max_n_nodes}_{job_idx}\\n\")\n",
        "        submit_file.write(f\"#SBATCH --output={job_type}{max_n_nodes}_{job_idx}o\\n\")\n",
        "        submit_file.write(f\"#SBATCH --time={runtime}\\n\")\n",
        "        submit_file.write(f\"#SBATCH --mem={mem}g\\n\")\n",
        "        submit_file.write(f\"#SBATCH --partition={ptn}\\n\")\n",
        "        submit_file.write(\"#SBATCH --nodes=1\\n\")\n",
        "        submit_file.write(f\"#SBATCH --cpus-per-task={cpu_per_task}\\n\")\n",
        "        if ptn == \"gpu\":\n",
        "            submit_file.write(\"#SBATCH --gres=gpu:1\\n\")\n",
        "        submit_file.write(\"hostname\\n\")\n",
        "        submit_file.write(\"export QT_QPA_PLATFORM='offscreen'\\n\")\n",
        "        submit_file.write(f\"{python_bin_path} {GRAPHINVENT_PATH}main.py --job-dir {job_dir}\")\n",
        "        submit_file.write(f\" > {job_dir}output.o${{SLURM_JOB_ID}}\\n\")\n",
        "\n",
        "\n",
        "def check_paths() -> None:\n",
        "    \"\"\"\n",
        "    Checks that paths to Python binary, data, and GraphINVENT are properly\n",
        "    defined before running a job, and tells the user to define them if not.\n",
        "    \"\"\"\n",
        "    for path in [PYTHON_PATH, GRAPHINVENT_PATH, DATA_PATH]:\n",
        "        if \"path/to/\" in path:\n",
        "            print(\"!!!\")\n",
        "            print(\"* Update the following paths in `submit.py` before running:\")\n",
        "            print(\"-- `PYTHON_PATH`\\n-- `GRAPHINVENT_PATH`\\n-- `DATA_PATH`\")\n",
        "            sys.exit(0)\n",
        "\n",
        "submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6-NrmmebHOU"
      },
      "source": [
        "# Generating New Molecules from the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lC3hd0XbGLh"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "pwd\n",
        "source activate graphinvent\n",
        "\n",
        "python3.8\n",
        "# load general packages and functions\n",
        "import csv\n",
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "import time\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# define what you want to do for the specified job(s)\n",
        "DATASET          = \"gdb13_1K-debug\"    # dataset name in \"./data/pre-training/\"\n",
        "JOB_TYPE         = \"generate\"          # \"preprocess\", \"train\", \"generate\", or \"test\"\n",
        "JOBDIR_START_IDX = 0                   # where to start indexing job dirs\n",
        "N_JOBS           = 1                   # number of jobs to run per model\n",
        "RESTART          = False               # whether or not this is a restart job\n",
        "FORCE_OVERWRITE  = True                # overwrite job directories which already exist\n",
        "JOBNAME          = \"example-job-name\"  # used to create a sub directory\n",
        "\n",
        "# if running using SLURM sbatch, specify params below\n",
        "USE_SLURM = False                        # use SLURM or not\n",
        "RUN_TIME  = \"1-00:00:00\"                 # hh:mm:ss\n",
        "MEM_GB    = 20                           # required RAM in GB\n",
        "\n",
        "# for SLURM jobs, set partition to run job on (preprocessing jobs run entirely on\n",
        "# CPU, so no need to request GPU partition; all other job types benefit from running\n",
        "# on a GPU)\n",
        "if JOB_TYPE == \"preprocess\":\n",
        "    PARTITION     = \"core\"\n",
        "    CPUS_PER_TASK = 1\n",
        "else:\n",
        "    PARTITION     = \"gpu\"\n",
        "    CPUS_PER_TASK = 4\n",
        "\n",
        "# set paths here\n",
        "HOME             = str(Path.home())\n",
        "PYTHON_PATH      = \"/usr/local/envs/graphinvent/bin/python3.8\"\n",
        "GRAPHINVENT_PATH = \"./graphinvent/\"\n",
        "DATA_PATH        = \"./data/pre-training/\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "\n",
        "# define dataset-specific parameters\n",
        "params = {\n",
        "    \"atom_types\"   : [\"C\", \"N\", \"O\", \"S\", \"Cl\"],\n",
        "    \"formal_charge\": [-1, 0, +1],\n",
        "    \"max_n_nodes\"  : 13,\n",
        "    \"job_type\"     : JOB_TYPE,\n",
        "    \"dataset_dir\"  : f\"{DATA_PATH}{DATASET}/\",\n",
        "    \"restart\"      : RESTART,\n",
        "    \"model\"        : \"GGNN\",\n",
        "    \"sample_every\" : 2,\n",
        "    \"init_lr\"      : 1e-4,\n",
        "    \"epochs\"       : 100,\n",
        "    \"batch_size\"   : 50,\n",
        "    \"block_size\"   : 1000,\n",
        "    \"device\"       : DEVICE,\n",
        "    \"n_samples\"    : 100,\n",
        "    # additional paramaters can be defined here, if different from the \"defaults\"\n",
        "    # for instance, for \"generate\" jobs, don't forget to specify \"generation_epoch\"\n",
        "    # and \"n_samples\"\n",
        "}\n",
        "\n",
        "\n",
        "def submit() -> None:\n",
        "    \"\"\"\n",
        "    Creates and submits submission script. Uses global variables defined at top\n",
        "    of this file.\n",
        "    \"\"\"\n",
        "    check_paths()\n",
        "\n",
        "    # create an output directory\n",
        "    dataset_output_path = f\"{HOME}/GraphINVENT/output_{DATASET}\"\n",
        "    tensorboard_path    = os.path.join(dataset_output_path, \"tensorboard\")\n",
        "    if JOBNAME != \"\":\n",
        "        dataset_output_path = os.path.join(dataset_output_path, JOBNAME)\n",
        "        tensorboard_path    = os.path.join(tensorboard_path, JOBNAME)\n",
        "\n",
        "    os.makedirs(dataset_output_path, exist_ok=True)\n",
        "    os.makedirs(tensorboard_path, exist_ok=True)\n",
        "    print(f\"* Creating dataset directory {dataset_output_path}/\", flush=True)\n",
        "\n",
        "    # submit `N_JOBS` separate jobs\n",
        "    jobdir_end_idx = JOBDIR_START_IDX + N_JOBS\n",
        "    for job_idx in range(JOBDIR_START_IDX, jobdir_end_idx):\n",
        "\n",
        "        # specify and create the job subdirectory if it does not exist\n",
        "        params[\"job_dir\"]         = f\"{dataset_output_path}/job_{job_idx}/\"\n",
        "        params[\"tensorboard_dir\"] = f\"{tensorboard_path}/job_{job_idx}/\"\n",
        "\n",
        "        # create the directory if it does not exist already, otherwise raises an\n",
        "        # error, which is good because *might* not want to override data our\n",
        "        # existing directories!\n",
        "        os.makedirs(params[\"tensorboard_dir\"], exist_ok=True)\n",
        "        try:\n",
        "            job_dir_exists_already = bool(\n",
        "                JOB_TYPE in [\"generate\", \"test\"] or FORCE_OVERWRITE\n",
        "            )\n",
        "            os.makedirs(params[\"job_dir\"], exist_ok=job_dir_exists_already)\n",
        "            print(\n",
        "                f\"* Creating model subdirectory {dataset_output_path}/job_{job_idx}/\",\n",
        "                flush=True,\n",
        "            )\n",
        "        except FileExistsError:\n",
        "            print(\n",
        "                f\"-- Model subdirectory {dataset_output_path}/job_{job_idx}/ already exists.\",\n",
        "                flush=True,\n",
        "            )\n",
        "            if not RESTART:\n",
        "                continue\n",
        "\n",
        "        # write the `input.csv` file\n",
        "        write_input_csv(params_dict=params, filename=\"input.csv\")\n",
        "\n",
        "        # write `submit.sh` and submit\n",
        "        if USE_SLURM:\n",
        "            print(\"* Writing submission script.\", flush=True)\n",
        "            write_submission_script(job_dir=params[\"job_dir\"],\n",
        "                                    job_idx=job_idx,\n",
        "                                    job_type=params[\"job_type\"],\n",
        "                                    max_n_nodes=params[\"max_n_nodes\"],\n",
        "                                    runtime=RUN_TIME,\n",
        "                                    mem=MEM_GB,\n",
        "                                    ptn=PARTITION,\n",
        "                                    cpu_per_task=CPUS_PER_TASK,\n",
        "                                    python_bin_path=PYTHON_PATH)\n",
        "\n",
        "            print(\"* Submitting job to SLURM.\", flush=True)\n",
        "            subprocess.run([\"sbatch\", params[\"job_dir\"] + \"submit.sh\"],\n",
        "                           check=True)\n",
        "        else:\n",
        "            print(\"* Running job as a normal process.\", flush=True)\n",
        "            subprocess.run([\"ls\", f\"{PYTHON_PATH}\"], check=True)\n",
        "            subprocess.run([f\"{PYTHON_PATH}\",\n",
        "                            f\"{GRAPHINVENT_PATH}main.py\",\n",
        "                            \"--job-dir\",\n",
        "                            params[\"job_dir\"]],\n",
        "                           check=True)\n",
        "\n",
        "        # sleep a few secs before submitting next job\n",
        "        print(\"-- Sleeping 2 seconds.\")\n",
        "        time.sleep(2)\n",
        "\n",
        "\n",
        "def write_input_csv(params_dict : dict, filename : str=\"params.csv\") -> None:\n",
        "    \"\"\"\n",
        "    Writes job parameters/hyperparameters in `params_dict` to CSV using the specified\n",
        "    `filename`.\n",
        "    \"\"\"\n",
        "    dict_path = params_dict[\"job_dir\"] + filename\n",
        "\n",
        "    with open(dict_path, \"w\") as csv_file:\n",
        "\n",
        "        writer = csv.writer(csv_file, delimiter=\";\")\n",
        "        for key, value in params_dict.items():\n",
        "            writer.writerow([key, value])\n",
        "\n",
        "\n",
        "def write_submission_script(job_dir : str, job_idx : int, job_type : str, max_n_nodes : int,\n",
        "                            runtime : str, mem : int, ptn : str, cpu_per_task : int,\n",
        "                            python_bin_path : str) -> None:\n",
        "    \"\"\"\n",
        "    Writes a submission script (`submit.sh`).\n",
        "\n",
        "    Args:\n",
        "    ----\n",
        "        job_dir (str)         : Job running directory.\n",
        "        job_idx (int)         : Job idx.\n",
        "        job_type (str)        : Type of job to run.\n",
        "        max_n_nodes (int)     : Maximum number of nodes in dataset.\n",
        "        runtime (str)         : Job run-time limit in hh:mm:ss format.\n",
        "        mem (int)             : Gigabytes to reserve.\n",
        "        ptn (str)             : Partition to use, either \"core\" (CPU) or \"gpu\" (GPU).\n",
        "        cpu_per_task (int)    : How many CPUs to use per task.\n",
        "        python_bin_path (str) : Path to Python binary to use.\n",
        "    \"\"\"\n",
        "    submit_filename = job_dir + \"submit.sh\"\n",
        "    with open(submit_filename, \"w\") as submit_file:\n",
        "        submit_file.write(\"#!/bin/bash\\n\")\n",
        "        submit_file.write(f\"#SBATCH --job-name={job_type}{max_n_nodes}_{job_idx}\\n\")\n",
        "        submit_file.write(f\"#SBATCH --output={job_type}{max_n_nodes}_{job_idx}o\\n\")\n",
        "        submit_file.write(f\"#SBATCH --time={runtime}\\n\")\n",
        "        submit_file.write(f\"#SBATCH --mem={mem}g\\n\")\n",
        "        submit_file.write(f\"#SBATCH --partition={ptn}\\n\")\n",
        "        submit_file.write(\"#SBATCH --nodes=1\\n\")\n",
        "        submit_file.write(f\"#SBATCH --cpus-per-task={cpu_per_task}\\n\")\n",
        "        if ptn == \"gpu\":\n",
        "            submit_file.write(\"#SBATCH --gres=gpu:1\\n\")\n",
        "        submit_file.write(\"hostname\\n\")\n",
        "        submit_file.write(\"export QT_QPA_PLATFORM='offscreen'\\n\")\n",
        "        submit_file.write(f\"{python_bin_path} {GRAPHINVENT_PATH}main.py --job-dir {job_dir}\")\n",
        "        submit_file.write(f\" > {job_dir}output.o${{SLURM_JOB_ID}}\\n\")\n",
        "\n",
        "\n",
        "def check_paths() -> None:\n",
        "    \"\"\"\n",
        "    Checks that paths to Python binary, data, and GraphINVENT are properly\n",
        "    defined before running a job, and tells the user to define them if not.\n",
        "    \"\"\"\n",
        "    for path in [PYTHON_PATH, GRAPHINVENT_PATH, DATA_PATH]:\n",
        "        if \"path/to/\" in path:\n",
        "            print(\"!!!\")\n",
        "            print(\"* Update the following paths in `submit.py` before running:\")\n",
        "            print(\"-- `PYTHON_PATH`\\n-- `GRAPHINVENT_PATH`\\n-- `DATA_PATH`\")\n",
        "            sys.exit(0)\n",
        "\n",
        "submit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2I9XYuluddJb"
      },
      "source": [
        "# Post Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOn_cNV9diIJ"
      },
      "outputs": [],
      "source": [
        "! for i in epochGEN{generation_epoch}_*.smi; do cat $i >> epochGEN{generation_epoch}.smi; done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dFOfkWVZp4V"
      },
      "source": [
        "## Filtering out invalid entries\n",
        "By default, GraphINVENT writes a \"Xe\" placeholder when an invalid molecular graph is generated, as an invalid molecular graph cannot be converted to a SMILES string for saving. The placeholder is used because the NLL is written for all generated graphs in a separate file, where the same line number in the *.nll file corresponds to the same line number in the *.smi file. Similarly, if an empty graph samples an invalid action as the first action, then no SMILES can be generated for an empty graph, so the corresponding line for an empty graph in a SMILES file contains only the \"ID\" of the molecule.\n",
        "\n",
        "For visualization, you might be interested in viewing only the valid molecular graphs. The SMILES for the generated molecules can thus be post-processed as follows to remove empty and invalid entries from a file before visualization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1Q-SnOYZsY7"
      },
      "outputs": [],
      "source": [
        "! sed -i \"/Xe/d\" /root/GraphINVENT/output_gdb13_1K-debug/example-job-name/job_0/generation/epoch_2.smi          # remove \"Xe\" placeholders from file\n",
        "! sed -i \"/^ [0-9]\\+$/d\" /root/GraphINVENT/output_gdb13_1K-debug/example-job-name/job_0/generation/epoch_2.smi  # remove empty graphs from file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0mFhjT2zcRz3"
      },
      "outputs": [],
      "source": [
        "#smi_file = \"/root/GraphINVENT/output_gdb13_1K-debug/example-job-name/job_0/generation/epoch_2.smi\"\n",
        "from google.colab import files\n",
        "files.download(\"/root/GraphINVENT/output_gdb13_1K-debug/example-job-name/job_0/generation/epoch_2.smi\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YQ0oZj8YLZJ"
      },
      "source": [
        "# Check the generated molcules\n",
        "\n",
        "Lets have a look at the generated molecules.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1ixX3LBiWlm0"
      },
      "outputs": [],
      "source": [
        "import pybel\n",
        "from IPython.display import SVG, display\n",
        "\n",
        "# Read molecules from the SMILES file\n",
        "molecules = [mol for mol in pybel.readfile(\"smi\", \"epoch_2.smi\")]\n",
        "\n",
        "# Visualize the first 5 molecules\n",
        "for mol in molecules[:5]:\n",
        "    svg = mol.write(\"svg\")\n",
        "    display(SVG(svg))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hg9EsrjWlyG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}